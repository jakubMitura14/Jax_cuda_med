{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Optional, Tuple, Type, List\n",
    "from jax import lax, random, numpy as jnp\n",
    "import einops\n",
    "import torchio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import jax\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "import einops\n",
    "import torchio as tio\n",
    "import optax\n",
    "from flax.training import train_state  \n",
    "from torch.utils.data import DataLoader\n",
    "import jax.profiler\n",
    "import ml_collections\n",
    "from ml_collections import config_dict\n",
    "from functools import partial\n",
    "import toolz\n",
    "import chex    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def filter_mask_of_intrest(masks,shift_x,shift_y):\n",
    "  masks= jnp.round(masks)\n",
    "  masks_a=(masks[:,:,0])==shift_x\n",
    "  masks_b=(masks[:,:,1])==shift_y\n",
    "  mask_0= jnp.logical_and(masks_a,masks_b).astype(int)    \n",
    "  return mask_0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_new_mask_from_probs(mask_old,bi_chan_probs,dim_stride,mask_shape,mask_shape_end,rearrange_to_intertwine_einops ):\n",
    "    \"\"\" \n",
    "    given bi channel probs where first channel will be interpreted as probability of being the same id (1 or 0)\n",
    "    as the sv backward the axis; and second channel probability of being the same supervoxel forward the axis \n",
    "    \"\"\"\n",
    "    #we add 1 becouse of batch\n",
    "    dim_stride_curr=dim_stride+1\n",
    "    #we get propositions forward and backward od old mask\n",
    "    old_forward=jnp.take(mask_old, indices=jnp.arange(1,mask_shape[dim_stride_curr]),axis=dim_stride_curr )\n",
    "    old_back =jnp.take(mask_old, indices=jnp.arange(0,mask_shape[dim_stride_curr]),axis=dim_stride_curr )\n",
    "    #in order to get correct shape we need to add zeros to forward\n",
    "    to_end_grid=jnp.ones(mask_shape_end)\n",
    "    old_forward= jnp.concatenate((old_forward,to_end_grid) ,axis= dim_stride_curr)\n",
    "\n",
    "    \n",
    "    old_propositions=jnp.stack([old_forward,old_back],axis=-1)# w h n_dim 2\n",
    "    #chosen values and its alternative\n",
    "    bi_chan_probs=jnp.round(bi_chan_probs)# TODO change into diff round - this is non differentiable !!   \n",
    "\n",
    "    bi_chan_probs=einops.repeat(bi_chan_probs,'w h pr->w h d pr',d=2)\n",
    "\n",
    "    chosen_values=jnp.multiply(old_propositions,bi_chan_probs)\n",
    "    chosen_values= jnp.sum(chosen_values,axis=-1)\n",
    "    chosen_values_alt=jnp.multiply(old_propositions,(jnp.flip(bi_chan_probs,axis=-1)))\n",
    "    chosen_values_alt= jnp.sum(chosen_values_alt,axis=-1)\n",
    "\n",
    "    \n",
    "    mask_combined=einops.rearrange([chosen_values,mask_old],rearrange_to_intertwine_einops) \n",
    "    mask_combined_alt=einops.rearrange([chosen_values_alt,mask_old],rearrange_to_intertwine_einops)\n",
    "\n",
    "    shift_x=0\n",
    "    shift_y=0\n",
    "    mask_combined_print=filter_mask_of_intrest(jnp.round(mask_combined[0,:,:,:]),shift_x,shift_y)\n",
    "    mask_old_print=filter_mask_of_intrest(jnp.round(mask_old[0,:,:,:]),shift_x,shift_y)\n",
    "    chosen_values_print=filter_mask_of_intrest(jnp.round(chosen_values[0,:,:,:]),shift_x,shift_y)\n",
    "    # jax.debug.print(\"{y} {z} \\n mask_combined_print \\n {a} \\n mask_old_print \\n{b} \\n chosen_values_print \\n {c} \\n *****************\", a=mask_combined_print ,b=mask_old_print, c=chosen_values_print ,y=shift_x, z= shift_y)\n",
    "\n",
    "    return mask_combined,mask_combined_alt\n",
    "dim_stride=0\n",
    "rearrange_to_intertwine_einops='f h w cc-> (h f) w cc'\n",
    "\n",
    "get_new_mask_from_probs(mask_old,bi_chan_probs,dim_stride,mask_shape,mask_shape_end,rearrange_to_intertwine_einops )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harder_diff_round(a):\n",
    "    return jnp.round(a)\n",
    "\n",
    "def differentiable_eq(a:float,b:float):\n",
    "    \"\"\"\n",
    "    will give big value if a anb b are similar and small otherwise\n",
    "    bot a and b are assumed to be between 0 and 1\n",
    "    \"\"\"\n",
    "    a= harder_diff_round(a)\n",
    "    b= harder_diff_round(b)\n",
    "    res=a*b+(1-a)*(1-b)\n",
    "    return harder_diff_round(res)\n",
    "\n",
    "def differentiable_and(a:float,b:float):\n",
    "    \"\"\"\n",
    "    will give big value if a anb b are similar and small otherwise\n",
    "    bot a and b are assumed to be between 0 and 1\n",
    "    \"\"\"\n",
    "    a= harder_diff_round(a)\n",
    "    b= harder_diff_round(b)\n",
    "    res=a*b\n",
    "    return harder_diff_round(res)\n",
    "\n",
    "\n",
    "\n",
    "#versions with second entry keeping as int\n",
    "v_differentiable_eq=jax.vmap(differentiable_eq,in_axes=(0,None))\n",
    "v_v_differentiable_eq=jax.vmap(v_differentiable_eq,in_axes=(0,None))\n",
    "v_v_v_differentiable_eq=jax.vmap(v_v_differentiable_eq,in_axes=(0,None))\n",
    "\n",
    "#version where both entries are 3 dimensional\n",
    "v_differentiable_eq_bi=jax.vmap(differentiable_eq,in_axes=(0,0))\n",
    "v_v_differentiable_eq_bi=jax.vmap(v_differentiable_eq_bi,in_axes=(0,0))\n",
    "v_v_v_differentiable_eq_bi=jax.vmap(v_v_differentiable_eq_bi,in_axes=(0,0))\n",
    "\n",
    "def filter_mask_of_intrest(mask,shift_x,shift_y):\n",
    "    \"\"\"\n",
    "    filters the mask to set to 1 only if it is this that we are currently intressted in \n",
    "\n",
    "    \"\"\"\n",
    "    coor_0_agree=v_v_differentiable_eq(mask[:,:,0],shift_x)\n",
    "    coor_1_agree=v_v_differentiable_eq(mask[:,:,1],shift_y)\n",
    "    print(f\"coor_0_agree {coor_0_agree} coor_1_agree {coor_1_agree} \")\n",
    "    return differentiable_and(coor_0_agree,coor_1_agree)  \n",
    "  \n",
    "dummy_mask=jnp.array([[[0.0,0.0],[0.0,1.0],[1.0,0.0],[1.0,1.0]]])\n",
    "\n",
    "filter_mask_of_intrest(dummy_mask, 0,1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_v_differentiable_eq_bi(jnp.array([[0.,0.,1.,1.]]),jnp.array([[1.,0.,1.,0.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diameter_no_pad(r):\n",
    "    \"\"\"\n",
    "    so every time we have n elements we can get n+ more elements\n",
    "    so analyzing on single axis\n",
    "    start from 1 ->1+1+1 =3 good\n",
    "    start from 3 ->3+3+1=7 good \n",
    "    start from 7 ->7+7+1=15 good \n",
    "    \"\"\"\n",
    "    curr = 1\n",
    "    for i in range(0,r):\n",
    "        curr=curr*2+1\n",
    "    return curr\n",
    "\n",
    "def get_diameter(r):\n",
    "    return get_diameter_no_pad(r)+1\n",
    "\n",
    "get_diameter(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_initial_supervoxel_masks(orig_grid_shape,shift_x,shift_y):\n",
    "    \"\"\"\n",
    "    on the basis of the present shifts we will initialize the masks\n",
    "    ids of the supervoxels here are implicit based on which mask and what location we are talking about\n",
    "    \"\"\"\n",
    "    initt=np.zeros(orig_grid_shape)\n",
    "    initt[shift_x::2,shift_y::2,0]=shift_x\n",
    "    initt[shift_x::2,shift_y::2,1]=shift_y\n",
    "\n",
    "    initt[shift_x::2,shift_y::4,2]=1\n",
    "    initt[shift_x::4,shift_y::2,3]=1\n",
    "\n",
    "    return initt\n",
    "\n",
    "def disp_to_pandas(probs,shappe ):\n",
    "    probs_to_disp= einops.rearrange(probs,'w h c-> (w h) c')\n",
    "    probs_to_disp=np.round(probs_to_disp,1)\n",
    "    # probs_to_disp=list(map(lambda twoo: f\"{twoo[0]} {twoo[1]}\",list(probs_to_disp)))\n",
    "    probs_to_disp=np.array(probs_to_disp)\n",
    "    probs_to_disp=list(map(lambda twoo: \" \".join(list(map(str,twoo))),list(probs_to_disp)))\n",
    "    probs_to_disp=np.array(probs_to_disp).reshape(shappe)\n",
    "    return pd.DataFrame(probs_to_disp)\n",
    "\n",
    "def disp_to_pandas_curr_shape(probs ):\n",
    "    return disp_to_pandas(probs,(probs.shape[0],probs.shape[1]) )\n",
    "\n",
    "\n",
    "cfg = config_dict.ConfigDict()\n",
    "cfg.total_steps=3\n",
    "cfg.batch_size=1\n",
    "# cfg.learning_rate=0.00002 #used for warmup with average coverage loss\n",
    "cfg.learning_rate=0.0001\n",
    "cfg.num_dim=4\n",
    "cfg.batch_size_pmapped=np.max([cfg.batch_size//jax.local_device_count(),1])\n",
    "cfg.img_size = (cfg.batch_size,1,64,64)\n",
    "# cfg.label_size = (cfg.batch_size,32,32)\n",
    "cfg.r_x_total= 3\n",
    "cfg.r_y_total= 3\n",
    "cfg.orig_grid_shape= (cfg.img_size[2]//2**cfg.r_x_total,cfg.img_size[3]//2**cfg.r_y_total,cfg.num_dim)\n",
    "cfg.epsilon=0.0000000001 \n",
    "cfg = ml_collections.FrozenConfigDict(cfg)\n",
    "\n",
    "initial_masks= jnp.stack([\n",
    "            get_initial_supervoxel_masks(cfg.orig_grid_shape,0,0),\n",
    "            get_initial_supervoxel_masks(cfg.orig_grid_shape,1,0),\n",
    "            get_initial_supervoxel_masks(cfg.orig_grid_shape,0,1),\n",
    "            get_initial_supervoxel_masks(cfg.orig_grid_shape,1,1)\n",
    "                ],axis=0)\n",
    "initial_masks=jnp.sum(initial_masks,axis=0).astype(int) \n",
    "disp_to_pandas_curr_shape(initial_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks_binary(masks,shift_x,shift_y,n_2,n_3):\n",
    "    masks_a=(masks[0,:,:,0])==shift_x\n",
    "    masks_b=(masks[0,:,:,1])==shift_y\n",
    "    masks_c=(masks[0,:,:,2])==n_2\n",
    "    masks_d=(masks[0,:,:,3])==n_3\n",
    "    mask_0_a= jnp.logical_and(masks_a,masks_b)   \n",
    "    mask_0_b= jnp.logical_and(masks_c,masks_d)\n",
    "    mask_0= jnp.logical_and(mask_0_a,mask_0_b)\n",
    "    return mask_0.astype(int) \n",
    "\n",
    "\n",
    "def get_summed_num_mask(masks_bin):\n",
    "    masks_bin=jnp.round(masks_bin)\n",
    "    res=(\n",
    "                 masks_binary(masks_bin,0,0,0,0)\n",
    "                +masks_binary(masks_bin,1,0,0,0)*2 \n",
    "                +masks_binary(masks_bin,0,1,0,0)*3 \n",
    "                +masks_binary(masks_bin,1,1,0,0)*4 \n",
    "                \n",
    "                +masks_binary(masks_bin,0,0,1,0)*5\n",
    "                +masks_binary(masks_bin,0,1,1,0)*6\n",
    "                +masks_binary(masks_bin,1,0,1,0)*7\n",
    "                +masks_binary(masks_bin,1,1,1,0)*8  \n",
    "                \n",
    "                +masks_binary(masks_bin,0,0,0,1)*9\n",
    "                +masks_binary(masks_bin,0,1,0,1)*10\n",
    "                +masks_binary(masks_bin,1,0,0,1)*11\n",
    "                +masks_binary(masks_bin,1,1,0,1)*12\n",
    "                \n",
    "                +masks_binary(masks_bin,0,0,1,1)*13\n",
    "                +masks_binary(masks_bin,0,1,1,1)*14\n",
    "                +masks_binary(masks_bin,1,0,1,1)*15\n",
    "                +masks_binary(masks_bin,1,1,1,1)*16)\n",
    "    return res\n",
    "    \n",
    "print(disp_to_pandas_curr_shape(initial_masks))\n",
    "print(get_summed_num_mask(einops.rearrange(initial_masks,'w h c-> 1 w h c')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harder_diff_round(a):\n",
    "    return jnp.round(a)\n",
    "\n",
    "def differentiable_eq(a:float,b:float):\n",
    "    \"\"\"\n",
    "    will give big value if a anb b are similar and small otherwise\n",
    "    bot a and b are assumed to be between 0 and 1\n",
    "    \"\"\"\n",
    "    a= harder_diff_round(a)\n",
    "    b= harder_diff_round(b)\n",
    "    res=a*b+(1-a)*(1-b)\n",
    "    return harder_diff_round(res)\n",
    "\n",
    "def differentiable_and(a:float,b:float):\n",
    "    a= harder_diff_round(a)\n",
    "    b= harder_diff_round(b)\n",
    "    res=a*b\n",
    "    return harder_diff_round(res)\n",
    "\n",
    "def filter_mask_of_intrest(mask,shift_x,shift_y):\n",
    "    \"\"\"\n",
    "    filters the mask to set to 1 only if it is this that we are currently intressted in \n",
    "    \"\"\"\n",
    "    coor_0_agree=v_v_differentiable_eq(mask[:,:,0],shift_x)\n",
    "    coor_1_agree=v_v_differentiable_eq(mask[:,:,1],shift_y)\n",
    "\n",
    "    # coor_0_agree=v_v_differentiable_eq(mask[:,:,0],shift_x)\n",
    "    # coor_1_agree=v_v_differentiable_eq(mask[:,:,1],shift_y)\n",
    "\n",
    "    return differentiable_and(coor_0_agree,coor_1_agree)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def a(x):\n",
    "    return jnp.sin(x*(jnp.pi/2))*jnp.sin(x*(jnp.pi/2))\n",
    "\n",
    "x = jnp.float32(1.0)\n",
    "\n",
    "print(jax.make_jaxpr(a)(x))\n",
    "# { lambda ; a:f32[]. let b:f32[] = sin a in (b,) }\n",
    "\n",
    "print(jax.make_jaxpr(jax.grad(a))(x))\n",
    "# { lambda ; a:f32[]. let\n",
    "#     _:f32[] = sin a\n",
    "#     b:f32[] = cos a\n",
    "#     c:f32[] = mul 1.0 b\n",
    "#   in (c,) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diff_round(x):\n",
    "    \"\"\"\n",
    "    differentiable version of round function\n",
    "    \"\"\"\n",
    "    # return x - jnp.sin(2*jnp.pi*x)/(2*jnp.pi)\n",
    "    # return jnp.sin(x*(jnp.pi/2))**2\n",
    "    return jnp.sin(x*(jnp.pi/2))*jnp.sin(x*(jnp.pi/2))\n",
    "\n",
    "def harder_diff_round(x):\n",
    "    return diff_round(diff_round(x))\n",
    "\n",
    "def differentiable_eq(a:float,b:float):\n",
    "    \"\"\"\n",
    "    will give big value if a anb b are similar and small otherwise\n",
    "    bot a and b are assumed to be between 0 and 1\n",
    "    \"\"\"\n",
    "    a= harder_diff_round(a)\n",
    "    b= harder_diff_round(b)\n",
    "    res=a*b+(1-a)*(1-b)\n",
    "    return harder_diff_round(res)\n",
    "\n",
    "def differentiable_and(a:float,b:float):\n",
    "    a= diff_round(a)\n",
    "    b= diff_round(b)\n",
    "    res=a*b\n",
    "    return res\n",
    "\n",
    "#versions with second entry keeping as int\n",
    "v_differentiable_eq=jax.vmap(differentiable_eq,in_axes=(0,None))\n",
    "v_v_differentiable_eq=jax.vmap(v_differentiable_eq,in_axes=(0,None))\n",
    "v_v_v_differentiable_eq=jax.vmap(v_v_differentiable_eq,in_axes=(0,None))\n",
    "\n",
    "def filter_mask_of_intrest(mask,initial_mask_id): #krowa test filter_mask_of_intrest\n",
    "    \"\"\"\n",
    "    filters the mask to set to 1 only if it is this that we are currently intressted in \n",
    "    \"\"\"\n",
    "    coor_0_agree=v_v_differentiable_eq(mask[:,:,0],initial_mask_id[0])\n",
    "    coor_1_agree=v_v_differentiable_eq(mask[:,:,1],initial_mask_id[1])\n",
    "    coor_2_agree=v_v_differentiable_eq(mask[:,:,2],initial_mask_id[2])\n",
    "    coor_3_agree=v_v_differentiable_eq(mask[:,:,3],initial_mask_id[3])\n",
    "    a=differentiable_and(coor_0_agree,coor_1_agree)\n",
    "    b=differentiable_and(coor_2_agree,coor_3_agree) \n",
    "\n",
    "    # coor_0_agree=v_v_differentiable_eq(mask[:,:,0],shift_x)\n",
    "    # coor_1_agree=v_v_differentiable_eq(mask[:,:,1],shift_y)\n",
    "\n",
    "    return differentiable_and(a,b)        \n",
    "\n",
    "mask=jnp.array([[[0.,0.,0.,0.],[1.,0.,0.,0.],[0.,1.,0.,0.],[1.,1.,0.,0.]\n",
    "                ,[0.,0.,1.,0.],[1.,0.,1.,0.],[0.,1.,1.,0.],[1.,1.,1.,0.]\n",
    "                ,[0.,0.,0.,1.],[1.,0.,0.,1.],[0.,1.,0.,1.],[1.,1.,0.,1.]\n",
    "                ,[0.,0.,1.,1.],[1.,0.,1.,1.],[0.,1.,1.,1.],[1.,1.,1.,1.]\n",
    "                  ]])\n",
    "\n",
    "shapp=mask.shape\n",
    "print(shapp)\n",
    "for i in range(shapp[1]):\n",
    "    print(filter_mask_of_intrest(mask, mask[0,i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_voxels.SIN.SIN_jax_2D_simpler.model_sin_jax_2D import SpixelNet\n",
    "from super_voxels.SIN.SIN_jax_2D_simpler.model_sin_jax_utils_2D import *\n",
    "from super_voxels.SIN.SIN_jax_2D_simpler.shape_reshape_functions import *\n",
    "from testUtils.spleenTest import get_spleen_data\n",
    "import h5py\n",
    "f = h5py.File('/workspaces/Jax_cuda_med/data/hdf5_loc/example_mask.hdf5', 'r+')\n",
    "masks=f['masks'][:,:,:]\n",
    "f.close()\n",
    "cached_subj =get_spleen_data()[0]\n",
    "masks= einops.rearrange(masks,'w h c->1 w h c')\n",
    "masks.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=cached_subj[0][0,:,64:-64,64:-64,32:33]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded from hdf5\n",
      "(1, 256)\n",
      "(1, 256, 16, 16, 4)\n",
      "(1, 256, 16, 16, 1)\n",
      "(1, 256, 4)\n"
     ]
    }
   ],
   "source": [
    "from super_voxels.SIN.SIN_jax_2D_simpler.model_sin_jax_2D import SpixelNet\n",
    "from super_voxels.SIN.SIN_jax_2D_simpler.model_sin_jax_utils_2D import *\n",
    "from super_voxels.SIN.SIN_jax_2D_simpler.shape_reshape_functions import *\n",
    "from testUtils.spleenTest import get_spleen_data\n",
    "import h5py\n",
    "\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Optional, Tuple, Type, List\n",
    "from jax import lax, random, numpy as jnp\n",
    "import einops\n",
    "import torchio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import jax\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "import einops\n",
    "import torchio as tio\n",
    "import optax\n",
    "from flax.training import train_state  \n",
    "from torch.utils.data import DataLoader\n",
    "import jax.profiler\n",
    "import ml_collections\n",
    "from ml_collections import config_dict\n",
    "from functools import partial\n",
    "import toolz\n",
    "import chex   \n",
    "f = h5py.File('/workspaces/Jax_cuda_med/data/hdf5_loc/example_mask.hdf5', 'r+')\n",
    "masks=f['masks'][:,:,:]\n",
    "f.close()\n",
    "cached_subj =get_spleen_data()[0]\n",
    "masks= einops.rearrange(masks,'w h c->1 w h c')\n",
    "masks.shape\n",
    "\n",
    "\n",
    "\n",
    "slicee=15\n",
    "image=cached_subj[0][0,0,64:-64,64:-64,slicee]\n",
    "image= einops.rearrange(image,'w h->1 w h 1')\n",
    "\n",
    "image.shape\n",
    "\n",
    "cfg = config_dict.ConfigDict()\n",
    "cfg.total_steps=300\n",
    "# cfg.learning_rate=0.00002 #used for warmup with average coverage loss\n",
    "cfg.learning_rate=0.0000001\n",
    "\n",
    "cfg.num_dim=4\n",
    "cfg.batch_size=160\n",
    "\n",
    "cfg.batch_size_pmapped=np.max([cfg.batch_size//jax.local_device_count(),1])\n",
    "cfg.img_size = (cfg.batch_size,1,256,256)\n",
    "cfg.label_size = (cfg.batch_size,256,256)\n",
    "cfg.r_x_total= 3\n",
    "cfg.r_y_total= 3\n",
    "cfg.orig_grid_shape= (cfg.img_size[2]//2**cfg.r_x_total,cfg.img_size[3]//2**cfg.r_y_total,cfg.num_dim)\n",
    "cfg.masks_num= 4# number of mask (4 in 2D and 8 in 3D)\n",
    "\n",
    "##getting the importance of the losses associated with deconvolutions\n",
    "## generally last one is most similar to the actual image - hence should be most important\n",
    "cfg.deconves_importances=(0.1,0.5,1.0)\n",
    "#some constant multipliers related to the fact that those losses are disproportionally smaller than the other ones\n",
    "cfg.edge_loss_multiplier=10.0\n",
    "cfg.feature_loss_multiplier=10.0\n",
    "#just for numerical stability\n",
    "cfg.epsilon=0.0000000001 \n",
    "cfg = ml_collections.FrozenConfigDict(cfg)\n",
    "dynamic_cfg_a = config_dict.ConfigDict()\n",
    "dynamic_cfg_a.is_beg=False\n",
    "dynamic_cfg_a = ml_collections.config_dict.FrozenConfigDict(dynamic_cfg_a)\n",
    "\n",
    "\n",
    "prng = jax.random.PRNGKey(42)\n",
    "dim_stride=0\n",
    "r_x=3\n",
    "r_y=3\n",
    "rearrange_to_intertwine_einops='f bb h w cc->bb (h f) w cc'\n",
    "translation_val=4\n",
    "features=32\n",
    "initial_masks= jnp.stack([\n",
    "            get_initial_supervoxel_masks(cfg.orig_grid_shape,0,0),\n",
    "            get_initial_supervoxel_masks(cfg.orig_grid_shape,1,0),\n",
    "            get_initial_supervoxel_masks(cfg.orig_grid_shape,0,1),\n",
    "            get_initial_supervoxel_masks(cfg.orig_grid_shape,1,1)\n",
    "                ],axis=0)\n",
    "initial_masks= jnp.sum(initial_masks,axis=0)\n",
    "initial_masks=einops.rearrange(initial_masks, 'a b c ->1 a b c')\n",
    "\n",
    "resized_image=image\n",
    "curried=(resized_image,masks,masks,initial_masks,masks,resized_image,initial_masks)\n",
    "\n",
    "\n",
    "model = De_conv_batched_for_scan(cfg,dynamic_cfg_a,dim_stride,r_x, r_y,rearrange_to_intertwine_einops,translation_val,features )\n",
    "params = model.init({'params': prng}, curried,0)#['params']\n",
    "new_curried,losses=model.apply(params, curried,0)\n",
    "resized_image,mask_combined,mask_combined_alt,initial_masks,mask_combined_new,resized_image_new,initial_masks_new=new_curried\n",
    "\n",
    "print(losses.shape)\n",
    "print(mask_combined_new.shape)\n",
    "print(resized_image_new.shape)\n",
    "print(initial_masks_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtoolz\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mchex\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrender2D\u001b[39;00m \u001b[39mimport\u001b[39;00m diff_round,Conv_trio\n\u001b[1;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscipy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjsp\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinen\u001b[39;00m \u001b[39mimport\u001b[39;00m partitioning \u001b[39mas\u001b[39;00m nn_partitioning\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "#based on https://github.com/yuanqqq/SIN\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Optional, Tuple, Type, List\n",
    "from jax import lax, random, numpy as jnp\n",
    "import einops\n",
    "import torchio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import jax\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "import einops\n",
    "import torchio as tio\n",
    "import optax\n",
    "from flax.training import train_state  \n",
    "from torch.utils.data import DataLoader\n",
    "import jax.profiler\n",
    "import ml_collections\n",
    "from ml_collections import config_dict\n",
    "from functools import partial\n",
    "import toolz\n",
    "import chex\n",
    "from .render2D import diff_round,Conv_trio\n",
    "import jax.scipy as jsp\n",
    "from flax.linen import partitioning as nn_partitioning\n",
    "from  .shape_reshape_functions import *\n",
    "remat = nn_partitioning.remat\n",
    "\n",
    "class De_conv_not_sym(nn.Module):\n",
    "    \"\"\"\n",
    "    Deconvolution plus activation function\n",
    "    strides may benon symmetric - and is controlled by dim_stride\n",
    "    dim_stride indicates which dimension should have stride =2\n",
    "    if dim_stride is -1 all dimension should be 2\n",
    "    \"\"\"\n",
    "    cfg: ml_collections.config_dict.config_dict.ConfigDict\n",
    "    features: int\n",
    "    dim_stride:int\n",
    "\n",
    "    def setup(self):\n",
    "        strides=[1,1]  \n",
    "        if(self.dim_stride==-1):\n",
    "            strides=[2,2]\n",
    "        strides[self.dim_stride]=2 \n",
    "        strides=tuple(strides)           \n",
    "        self.convv = nn.ConvTranspose(\n",
    "                features=self.features,\n",
    "                kernel_size=(5, 5),\n",
    "                strides=strides,              \n",
    "                )\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        x=self.convv(x)\n",
    "        return jax.nn.gelu(x)\n",
    "    \n",
    "def harder_diff_round(x):\n",
    "    # return diff_round(diff_round(x))\n",
    "    return diff_round(diff_round(x))\n",
    "\n",
    "    # return diff_round(diff_round(x))\n",
    "    # return  diff_round(diff_round(diff_round(diff_round(diff_round(diff_round(diff_round(diff_round(diff_round(diff_round(diff_round(diff_round(diff_round(x)))))))))))))\n",
    "    # - 0.51 so all \n",
    "    # return diff_round(diff_round(nn.relu(x-0.51)))\n",
    "    # return nn.softmax(jnp.power((x)+1,14))\n",
    "\n",
    "v_harder_diff_round=jax.vmap(harder_diff_round)\n",
    "v_v_harder_diff_round=jax.vmap(v_harder_diff_round)\n",
    "v_v_v_harder_diff_round=jax.vmap(v_v_harder_diff_round)\n",
    "\n",
    "    \n",
    "def masked_cross_entropy_loss(logits: jnp.ndarray,\n",
    "                       one_hot_labels: jnp.ndarray,\n",
    "                       mask: Optional[jnp.ndarray] = None) -> jnp.ndarray:\n",
    "  \"\"\"\n",
    "  based on https://github.com/google-research/sam/blob/main/sam_jax/training_utils/flax_training.py\n",
    "  Returns the cross entropy loss between some logits and some labels.\n",
    "  Args:\n",
    "    logits: Output of the model.\n",
    "    one_hot_labels: One-hot encoded labels. Dimensions should match the logits.\n",
    "    mask: Mask to apply to the loss to ignore some samples (usually, the padding\n",
    "      of the batch). Array of ones and zeros.\n",
    "  Returns:\n",
    "    The cross entropy, averaged over the first dimension (samples).\n",
    "  \"\"\"\n",
    "  log_softmax_logits = jax.nn.log_softmax(logits)\n",
    "#   mask = mask.reshape([logits.shape[0], 1])\n",
    "  loss = -jnp.sum(one_hot_labels * log_softmax_logits * mask) / mask.sum()\n",
    "  return jnp.nan_to_num(loss)  # Set to zero if there is no non-masked samples.    \n",
    "\n",
    "def losss(prob_plane,label_plane):\n",
    "    \"\"\"\n",
    "    we will compare each plane in the axis the we did strided deconvolution\n",
    "    as is important we have two channels here\n",
    "    one channel will be interpreted as the probability that the voxel up the axis is the same class\n",
    "    other channels will look for the same down axis\n",
    "\n",
    "    than we need to compare it to the same information in the label - gold standard\n",
    "    however ignoring all of the spots where label is 0 - not all voxels needs to be segmented in gold standard\n",
    "    prob_plane - 2 channel planes with probability up and down\n",
    "    label_plane - 2 channel float plane about gold standard - first channel tell is voxel the same class \n",
    "        as up in axis , second down  \n",
    "    \"\"\"\n",
    "    \n",
    "    return masked_cross_entropy_loss(nn.sigmoid(prob_plane),label_plane,label_plane)\n",
    "    # return optax.sigmoid_binary_cross_entropy(prob_plane, label_plane)\n",
    "@partial(jax.profiler.annotate_function, name=\"check_mask_consistency\")\n",
    "def check_mask_consistency(mask_old,mask_new,axis,epsilon):\n",
    "    \"\"\"\n",
    "    as we are simulating interpolation we want to check weather the new mask behaves as\n",
    "    the interpolation of old one - so the new entries of the mask should be similar to the entries up ro down axis \n",
    "    so basically we want to get a similar values when we go 1 up and one down the axis\n",
    "    However we have the special case of the edge, \n",
    "    in order to detect the edge and treat it separately we will use image gradients in a given direction\n",
    "    \"\"\"\n",
    "    mutliplier=3\n",
    "    #we pad becouse on the border we have high gradient - and we want to ignore it\n",
    "    old_mask_padded= jnp.pad(mask_old,((1,1),(1,1)))\n",
    "    #as some entries will be negative oter positive we square it\n",
    "    grad = jnp.power(jnp.gradient(old_mask_padded,axis=axis),2)\n",
    "    #removing unnecessery padding\n",
    "    grad= grad[1:-1,1:-1]\n",
    "    #we multiply to make sure that we will get values above 1 - we will relu later so exact values do not matter\n",
    "    #we have here the old mask dilatated in place of edges in a chosen axis\n",
    "    sum_grads= grad*mutliplier+mask_old*mutliplier\n",
    "    #now we subtract from the new mask old one so all positive value that remain are in the spots \n",
    "    #that should not be present\n",
    "    for_loss=mask_new-sum_grads\n",
    "    #we get rid of negative values - as those should not contribute to loss\n",
    "    # for_loss=jnp.exp(for_loss)\n",
    "    for_loss=jnp.power(2,for_loss)\n",
    "    # for_loss=nn.relu(for_loss)\n",
    "    #we sum all of the positive entries - the bigger the sum the worse is the consistency\n",
    "    # now we need also to get in opposite side so we assume shrinking instead of enlarging and check weather a new version agrees with a shrinked version ...\n",
    "    #so only positive should be shrinken version of old mask\n",
    "    diff_grads= mask_old-grad*mutliplier\n",
    "    # diff_grads=jnp.exp(diff_grads)\n",
    "    diff_grads=jnp.power(2,diff_grads)\n",
    "    # diff_grads=nn.relu(diff_grads)\n",
    "    #now we want to make sure that where diff grads are big the new mask is also\n",
    "    should_be_high= jnp.multiply(diff_grads,mask_new)\n",
    "    \n",
    "    should_low=jnp.mean(for_loss.flatten())+epsilon\n",
    "    should_high=jnp.mean(should_be_high.flatten())+epsilon\n",
    "\n",
    "    return should_high/(should_high+should_low)\n",
    "\n",
    "\n",
    "def translate_mask_in_axis(mask:jnp.ndarray, axis:int,is_forward:int,translation_val:int,mask_shape:Tuple[int]):\n",
    "    \"\"\"\n",
    "    translates the mask in a given axis \n",
    "    also forward or backward it perform it by padding and\n",
    "    take\n",
    "    value of translation is described by translation_val\n",
    "    \"\"\"\n",
    "    mask_orig=mask.copy()\n",
    "    mask= jnp.take(mask, indices=jnp.arange(translation_val*(1-is_forward),mask_shape[axis]-translation_val* is_forward),axis=axis )\n",
    "    to_pad=np.array([[0,0],[0,0]])\n",
    "    is_back=1-is_forward\n",
    "    to_pad[axis,is_back]=translation_val\n",
    "\n",
    "    mask= jnp.pad(mask,to_pad)\n",
    "    return jnp.multiply(mask,mask_orig)\n",
    "\n",
    "def get_image_features(image:jnp.ndarray,mask:jnp.ndarray,epsilon:float):\n",
    "    \"\"\"\n",
    "    given image and a mask will calculate the set of image features\n",
    "    that will return as a vector  \n",
    "    \"\"\"\n",
    "    masked_image= jnp.multiply(image,mask)\n",
    "    meann= jnp.sum(masked_image.flatten())/(jnp.sum(mask.flatten())+epsilon)\n",
    "    varr= jnp.power( jnp.multiply((masked_image-meann),mask  ),2)\n",
    "    varr=jnp.sum(varr.flatten())/(jnp.sum(mask.flatten())+epsilon)\n",
    "    return jnp.array([meann, varr])\n",
    "\n",
    "def get_translated_mask_variance(image:jnp.ndarray\n",
    "                                 ,mask:jnp.ndarray\n",
    "                                 ,translation_val:int\n",
    "                                 ,mask_shape:Tuple[int]\n",
    "                                 ,feature_loss_multiplier:float\n",
    "                                 ,epsilon:float):\n",
    "    \"\"\" \n",
    "    we will make a translation of the mask in all directions and check wether image features change\n",
    "    generally the same supervoxel should have the same image features in all of its subregions\n",
    "    so we want the variance here to be small \n",
    "    \"\"\"\n",
    "    image=einops.rearrange(image,'w h c-> w (h c)')\n",
    "    features=jnp.stack([\n",
    "        get_image_features(image,translate_mask_in_axis(mask,0,0,translation_val,mask_shape),epsilon),\n",
    "        get_image_features(image,translate_mask_in_axis(mask,0,1,translation_val,mask_shape),epsilon),\n",
    "        get_image_features(image,translate_mask_in_axis(mask,1,0,translation_val,mask_shape),epsilon),\n",
    "        get_image_features(image,translate_mask_in_axis(mask,1,1,translation_val,mask_shape),epsilon)\n",
    "              ])\n",
    "    # maxes= jnp.max(features,axis=0)\n",
    "    # features=features/maxes\n",
    "    feature_variance=jnp.var(features,axis=0)*feature_loss_multiplier\n",
    "    return jnp.mean(feature_variance)\n",
    "    # return jnp.var(jnp.multiply(image,mask).flatten())\n",
    "\n",
    "\n",
    "@partial(jax.profiler.annotate_function, name=\"get_edgeloss\")\n",
    "def get_edgeloss(image:jnp.ndarray,mask:jnp.ndarray,axis:int,edge_loss_multiplier:float):\n",
    "    \"\"\"\n",
    "    in order to also force the supervoxels to keep to the strong edges\n",
    "    we can add edge loss that will be comparing a directional gradient of the mask\n",
    "    and image, hovewer the importance of the loss should be proportional to the strength of the edges\n",
    "    so we can simply get first the l2 loss element wise than scale it by the image gradient\n",
    "    \"\"\"\n",
    "    image_gradient=jnp.gradient(image,axis=axis)#*edge_loss_multiplier\n",
    "    mask_gradient=jnp.gradient(mask,axis=axis)#*edge_loss_multiplier\n",
    "    # image_gradient=image_gradient/jnp.max(image_gradient.flatten())\n",
    "    # mask_gradient=mask_gradient/jnp.max(mask_gradient.flatten())\n",
    "\n",
    "    element_wise_l2=optax.l2_loss(image_gradient,mask_gradient)\n",
    "    # element_wise_l2= jnp.multiply(element_wise_l2,image_gradient)*edge_loss_multiplier\n",
    "    return jnp.mean(element_wise_l2.flatten())*(-1)\n",
    "\n",
    "def rounding_loss(arr):\n",
    "    \"\"\"\n",
    "    it is minimized when all entries are either 0 or 1\n",
    "    \"\"\"\n",
    "    return 1.00000001-jnp.mean(jnp.power(arr-(1-arr),2) )\n",
    "\n",
    "def differentiable_eq(a:float,b:float):\n",
    "    \"\"\"\n",
    "    will give big value if a anb b are similar and small otherwise\n",
    "    bot a and b are assumed to be between 0 and 1\n",
    "    \"\"\"\n",
    "    a= harder_diff_round(a)\n",
    "    b= harder_diff_round(b)\n",
    "    res=a*b+(1-a)*(1-b)\n",
    "    return harder_diff_round(res)\n",
    "\n",
    "def differentiable_and(a:float,b:float):\n",
    "    a= diff_round(a)\n",
    "    b= diff_round(b)\n",
    "    res=a*b\n",
    "    return res\n",
    "\n",
    "#versions with second entry keeping as int\n",
    "v_differentiable_eq=jax.vmap(differentiable_eq,in_axes=(0,None))\n",
    "v_v_differentiable_eq=jax.vmap(v_differentiable_eq,in_axes=(0,None))\n",
    "v_v_v_differentiable_eq=jax.vmap(v_v_differentiable_eq,in_axes=(0,None))\n",
    "\n",
    "#version where both entries are 3 dimensional\n",
    "v_differentiable_and_bi=jax.vmap(differentiable_and,in_axes=(0,0))\n",
    "v_v_differentiable_and_bi=jax.vmap(v_differentiable_and_bi,in_axes=(0,0))\n",
    "v_v_v_differentiable_and_bi=jax.vmap(v_v_differentiable_and_bi,in_axes=(0,0))\n",
    "\n",
    "\n",
    "def filter_mask_of_intrest(mask,initial_mask_id): \n",
    "    \"\"\"\n",
    "    filters the mask to set to 1 only if it is this that we are currently intressted in \n",
    "    \"\"\"\n",
    "    coor_0_agree=v_v_differentiable_eq(mask[:,:,0],initial_mask_id[0])\n",
    "    coor_1_agree=v_v_differentiable_eq(mask[:,:,1],initial_mask_id[1])\n",
    "    coor_2_agree=v_v_differentiable_eq(mask[:,:,2],initial_mask_id[2])\n",
    "    coor_3_agree=v_v_differentiable_eq(mask[:,:,3],initial_mask_id[3])\n",
    "    a=differentiable_and(coor_0_agree,coor_1_agree)\n",
    "    b=differentiable_and(coor_2_agree,coor_3_agree) \n",
    "\n",
    "    # coor_0_agree=v_v_differentiable_eq(mask[:,:,0],shift_x)\n",
    "    # coor_1_agree=v_v_differentiable_eq(mask[:,:,1],shift_y)\n",
    "\n",
    "    return differentiable_and(a,b)        \n",
    "\n",
    "\n",
    "class Apply_on_single_area(nn.Module):\n",
    "    \"\"\"\n",
    "    module will be vmapped or scanned over all supervoxel areas\n",
    "    for simplicity of implementation here we are just working on single supervoxel area\n",
    "    \"\"\"\n",
    "    cfg: ml_collections.config_dict.config_dict.ConfigDict\n",
    "    dynamic_cfg: ml_collections.config_dict.config_dict.ConfigDict\n",
    "    rearrange_to_intertwine_einops:str\n",
    "    dim_stride:int\n",
    "    curr_shape:Tuple[int]\n",
    "    deconved_shape:Tuple[int]\n",
    "    translation_val:int\n",
    "    diameter_x:int\n",
    "    diameter_y:int\n",
    "    diameter_x_curr:int\n",
    "    diameter_y_curr:int\n",
    "    p_x:int\n",
    "    p_y:int\n",
    "    to_reshape_back_x:int\n",
    "    to_reshape_back_y:int    \n",
    "    def setup(self):\n",
    "        self.mask_shape=(self.diameter_x_curr,self.diameter_y_curr,self.cfg.num_dim)        \n",
    "        mask_shape_list=list(self.mask_shape)\n",
    "        mask_shape_list[self.dim_stride]=1\n",
    "        self.mask_shape_end=tuple(mask_shape_list)\n",
    "\n",
    "\n",
    "    # @partial(jax.profiler.annotate_function, name=\"select_set_non_overlapping_regions\")\n",
    "    # def select_set_non_overlapping_regions(self,index):\n",
    "    #     \"\"\"\n",
    "    #     in order are for shift_x,shift_y\n",
    "    #     0) 0 0\n",
    "    #     1) 1 0\n",
    "    #     2) 0 1\n",
    "    #     3) 1 1\n",
    "    #     \"\"\"        \n",
    "    #     def fun_ff():\n",
    "    #         return set_non_overlapping_regions(self.diameter_x,self.diameter_y,0,0,self.p_x,self.p_y)\n",
    "            \n",
    "    #     def fun_tf():\n",
    "    #         return set_non_overlapping_regions(self.diameter_x,self.diameter_y,1,0,self.p_x,self.p_y)\n",
    "            \n",
    "    #     def fun_ft():\n",
    "    #         return set_non_overlapping_regions(self.diameter_x,self.diameter_y,0,1,self.p_x,self.p_y)\n",
    "\n",
    "    #     def fun_tt():\n",
    "    #         return set_non_overlapping_regions(self.diameter_x,self.diameter_y,1,1,self.p_x,self.p_y)\n",
    "\n",
    "    #     functions_list=[fun_ff,fun_tf,fun_ft,fun_tt]\n",
    "    #     return jax.lax.switch(index,functions_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_feature_var_loss(self,resized_image,mask_combined,mask_combined_alt,epsilon, initial_mask_id):\n",
    "        mask_combined_curr=filter_mask_of_intrest(mask_combined_alt,initial_mask_id)\n",
    "\n",
    "        #using alternative mask - that should be worse then main version\n",
    "        feature_variance_loss_alt=get_translated_mask_variance(resized_image, mask_combined_curr\n",
    "                                                        ,self.translation_val, (self.diameter_x,\n",
    "                                                                                self.diameter_y ) ,self.cfg.feature_loss_multiplier,self.cfg.epsilon)\n",
    "\n",
    "\n",
    "        #using main mask\n",
    "        mask_combined_curr=filter_mask_of_intrest(mask_combined,initial_mask_id)\n",
    "\n",
    "        feature_variance_loss_main=get_translated_mask_variance(resized_image, mask_combined_curr\n",
    "                                                        ,self.translation_val, (self.diameter_x,\n",
    "                                                                                self.diameter_y ) ,self.cfg.feature_loss_multiplier,self.cfg.epsilon )\n",
    "         #feature_variance_loss_main should be small becouse we want to minimize the variance of features in chosen supervoxel\n",
    "        #feature_variance_loss_alt should be big\n",
    "\n",
    "        #such calculation will lead to be in range 0-1\n",
    "        feature_variance_loss=feature_variance_loss_main/((feature_variance_loss_main+feature_variance_loss_alt) +epsilon)\n",
    "\n",
    "\n",
    "\n",
    "        # return feature_variance_loss krowa TODO(unhash)\n",
    "        return feature_variance_loss_main\n",
    "\n",
    "\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self\n",
    "                 ,resized_image:jnp.ndarray\n",
    "                ,mask_combined:jnp.ndarray\n",
    "                ,mask_combined_alt:jnp.ndarray\n",
    "                ,mask_index:int\n",
    "                 ,initial_mask_id:jnp.ndarray ) -> jnp.ndarray:\n",
    "\n",
    "        #calculate image feature variance in the supervoxel itself\n",
    "        # feature_variance_loss,edgeloss,mask_combined=self.get_feature_va_and_edge_loss(resized_image,chosen_values,chosen_values_alt,mask_old,self.cfg.epsilon)\n",
    "        \n",
    "        # mask_combined=mask_combined.at[:,-1].set(0) \n",
    "        # mask_combined=mask_combined.at[-1,:].set(0)        \n",
    "        # shift_x= jnp.remainder(mask_index,2)\n",
    "        # shift_y=mask_index//2\n",
    "        # mask_combined_print=filter_mask_of_intrest(jnp.round(mask_combined),shift_x,shift_y)*(mask_index+1)+((area_index+1)*100)\n",
    "\n",
    "        return self.get_feature_var_loss(resized_image,mask_combined,mask_combined_alt,self.cfg.epsilon,initial_mask_id)\n",
    "\n",
    "\n",
    "v_Apply_on_single_area=nn.vmap(Apply_on_single_area\n",
    "                            ,in_axes=(0, 0,0,None,0)\n",
    "                            ,out_axes=0\n",
    "                            ,variable_axes={'params': None} #parametters are shared\n",
    "                            ,split_rngs={'params': False}\n",
    "                            )\n",
    "\n",
    "batched_v_Apply_on_single_area=nn.vmap(v_Apply_on_single_area\n",
    "                            ,in_axes=(0, 0,0,None,0)\n",
    "                            ,out_axes=0\n",
    "                            ,variable_axes={'params': None} #parametters are shared\n",
    "                            ,split_rngs={'params': False}\n",
    "                            )\n",
    "\n",
    "v_image_resize=jax.vmap(jax.image.resize,in_axes=(0,None,None))\n",
    "\n",
    "def masks_binary(masks,shift_x,shift_y,n_2,n_3):\n",
    "    masks_a=(masks[0,:,:,0])==shift_x\n",
    "    masks_b=(masks[0,:,:,1])==shift_y\n",
    "    masks_c=(masks[0,:,:,2])==n_2\n",
    "    masks_d=(masks[0,:,:,3])==n_3\n",
    "    mask_0_a= jnp.logical_and(masks_a,masks_b)   \n",
    "    mask_0_b= jnp.logical_and(masks_c,masks_d)\n",
    "    mask_0= jnp.logical_and(mask_0_a,mask_0_b)\n",
    "    return mask_0.astype(int) \n",
    "\n",
    "\n",
    "def get_summed_num_mask(masks_bin):\n",
    "    masks_bin=jnp.round(masks_bin)\n",
    "    res=(\n",
    "                 masks_binary(masks_bin,0,0,0,0)\n",
    "                +masks_binary(masks_bin,1,0,0,0)*2 \n",
    "                +masks_binary(masks_bin,0,1,0,0)*3 \n",
    "                +masks_binary(masks_bin,1,1,0,0)*4 \n",
    "                \n",
    "                +masks_binary(masks_bin,0,0,1,0)*5\n",
    "                +masks_binary(masks_bin,0,1,1,0)*6\n",
    "                +masks_binary(masks_bin,1,0,1,0)*7\n",
    "                +masks_binary(masks_bin,1,1,1,0)*8  \n",
    "                \n",
    "                +masks_binary(masks_bin,0,0,0,1)*9\n",
    "                +masks_binary(masks_bin,0,1,0,1)*10\n",
    "                +masks_binary(masks_bin,1,0,0,1)*11\n",
    "                +masks_binary(masks_bin,1,1,0,1)*12\n",
    "                \n",
    "                +masks_binary(masks_bin,0,0,1,1)*13\n",
    "                +masks_binary(masks_bin,0,1,1,1)*14\n",
    "                +masks_binary(masks_bin,1,0,1,1)*15\n",
    "                +masks_binary(masks_bin,1,1,1,1)*16)\n",
    "    return res\n",
    "\n",
    "\n",
    "# def get_mask_with_num(bin_mask,shift_x,shift_y,cfg,r_x,r_y):\n",
    "#     # print(f\"aa bin_mask {bin_mask.shape}\")\n",
    "#     bin_mask=jnp.round(bin_mask)\n",
    "#     bin_mask=masks_binary(bin_mask,shift_x,shift_y)\n",
    "#     sh_r=get_shape_reshape_constants(cfg,shift_x,shift_y,r_x,r_y)\n",
    "#     bin_mask= einops.rearrange(bin_mask,'w h -> 1 w h 1')\n",
    "#     # print(f\"bb bin_mask {bin_mask.shape}\")\n",
    "\n",
    "#     mask_now=divide_sv_grid(bin_mask,sh_r)\n",
    "#     b_dim,pp_dim,w_dim,h_dim,c_dim=mask_now.shape\n",
    "#     aranged= jnp.arange(1,pp_dim+1)\n",
    "#     aranged=einops.repeat(aranged,'pp->b pp w h c',b=b_dim,w = w_dim,h= h_dim,c= c_dim)\n",
    "#     # print(f\"aranged \\n {disp_to_pandas_curr_shape(aranged[0,:,:,0])} \\n\")\n",
    "#     res = jnp.multiply(mask_now,aranged)\n",
    "#     a=sh_r.axis_len_x//sh_r.diameter_x\n",
    "#     b=sh_r.axis_len_y//sh_r.diameter_y\n",
    "#     return recreate_orig_shape(res,sh_r,a,b)[0,:,:,0]\n",
    "\n",
    "# def get_summed_num_mask(masks_binary,cfg,r_x,r_y):\n",
    "#     mask_0_num=get_mask_with_num(masks_binary,0,0,cfg,r_x,r_y)\n",
    "#     mask_1_num=get_mask_with_num(masks_binary,1,0,cfg,r_x,r_y)\n",
    "#     mask_2_num=get_mask_with_num(masks_binary,0,1,cfg,r_x,r_y)\n",
    "#     mask_3_num=get_mask_with_num(masks_binary,1,1,cfg,r_x,r_y)\n",
    "#     # return (mask_0_num+10*(mask_0_num>0)) +(mask_1_num+20*(mask_1_num>0))+(mask_2_num+30*(mask_2_num>0))+(mask_3_num+40*(mask_3_num>0))\n",
    "#     return mask_0_num+mask_1_num+mask_2_num+mask_3_num\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class De_conv_batched_for_scan(nn.Module):\n",
    "    \"\"\"\n",
    "    Created for scanning over masks\n",
    "    as all configurations True False in shifts in axis is \n",
    "    masks in order are for shift_x,shift_y\n",
    "    0) 0 0\n",
    "    1) 1 0\n",
    "    2) 0 1\n",
    "    3) 1 1\n",
    "    \"\"\"\n",
    "\n",
    "    cfg: ml_collections.config_dict.config_dict.ConfigDict\n",
    "    dynamic_cfg: ml_collections.config_dict.config_dict.ConfigDict\n",
    "    dim_stride:int\n",
    "    r_x:int\n",
    "    r_y:int\n",
    "    rearrange_to_intertwine_einops:str\n",
    "    translation_val:int\n",
    "    features:int\n",
    "\n",
    "    def setup(self):\n",
    "        cfg=self.cfg\n",
    "        #we will calculate \n",
    "        rss=[self.r_x,self.r_y]\n",
    "        rss[self.dim_stride]=rss[self.dim_stride]-1\n",
    "        self.rss=rss\n",
    "        #we get image size 2 and 3 becouse 0 and 1 is batch and channel\n",
    "        self.deconved_shape = (self.cfg.batch_size_pmapped,cfg.img_size[2]//2**(cfg.r_x_total -self.r_x),cfg.img_size[3]//2**(cfg.r_y_total-self.r_y),1)\n",
    "        self.current_shape = (self.cfg.batch_size_pmapped,cfg.img_size[2]//2**(cfg.r_x_total-rss[0]),cfg.img_size[3]//2**(cfg.r_y_total-rss[1]))\n",
    "        \n",
    "\n",
    "        # masks in order are for shift_x,shift_y\n",
    "        # 0) 0 0\n",
    "        # 1) 1 0\n",
    "        # 2) 0 1\n",
    "        # 3) 1 1        \n",
    "        self.shape_reshape_cfgs=get_all_shape_reshape_constants(self.cfg,r_x=self.r_x,r_y=self.r_y)\n",
    "        self.shape_reshape_cfg_olds=get_all_shape_reshape_constants(self.cfg,r_x=rss[0], r_y=rss[1])\n",
    "\n",
    "        self.diameter_x=self.shape_reshape_cfgs[0].diameter_x #get_diameter_no_pad(cfg.r_x_total -self.r_x)\n",
    "        self.diameter_y=self.shape_reshape_cfgs[0].diameter_y #get_diameter_no_pad(cfg.r_y_total-self.r_y)# self.shape_reshape_cfgs[0].diameter_y\n",
    "        \n",
    "        self.orig_grid_shape=self.shape_reshape_cfgs[0].orig_grid_shape\n",
    "        \n",
    "        self.diameter_x_curr=self.shape_reshape_cfg_olds[0].diameter_x #get_diameter_no_pad(cfg.r_x_total -self.r_x)\n",
    "        self.diameter_y_curr=self.shape_reshape_cfg_olds[0].diameter_y #get_diameter_no_pad(cfg.r_y_total-self.r_y)# self.shape_reshape_cfgs[0].diameter_y\n",
    "                \n",
    "        self.axis_len_x=self.shape_reshape_cfgs[0].axis_len_x #get_diameter_no_pad(cfg.r_y_total-self.r_y)# self.shape_reshape_cfgs[0].diameter_y\n",
    "        self.axis_len_y=self.shape_reshape_cfgs[0].axis_len_y #get_diameter_no_pad(cfg.r_y_total-self.r_y)# self.shape_reshape_cfgs[0].diameter_y\n",
    "\n",
    "\n",
    "        self.p_x_y=np.array([np.maximum(((self.diameter_x-1)//2)-1,0),np.maximum(((self.diameter_y-1)//2)-1,0)])#-shape_reshape_cfg.shift_y\n",
    "        self.p_x_y= (self.p_x_y[0],self.p_x_y[1])\n",
    "        self.to_reshape_back_x=np.floor_divide(self.axis_len_x,self.diameter_x)\n",
    "        self.to_reshape_back_y=np.floor_divide(self.axis_len_y,self.diameter_y)\n",
    "\n",
    "       \n",
    "\n",
    "    def select_shape_reshape_operation(self,arr,index,shape_reshape_cfgs,fun):\n",
    "        \"\"\"\n",
    "        in order are for shift_x,shift_y\n",
    "        0) 0 0\n",
    "        1) 1 0\n",
    "        2) 0 1\n",
    "        3) 1 1\n",
    "        \"\"\"\n",
    "        def fun_ff():\n",
    "            return fun(arr,shape_reshape_cfgs[0])\n",
    "            \n",
    "        def fun_tf():\n",
    "            return fun(arr,shape_reshape_cfgs[1])\n",
    "            \n",
    "        def fun_ft():\n",
    "            return fun(arr,shape_reshape_cfgs[2])\n",
    "\n",
    "        def fun_tt():\n",
    "            return fun(arr,shape_reshape_cfgs[3])\n",
    "\n",
    "        functions_list=[fun_ff,fun_tf,fun_ft,fun_tt]\n",
    "        return jax.lax.switch(index,functions_list)\n",
    "\n",
    "    def select_id_choose_operation(self,initial_masks,index,shape_reshape_cfgs):\n",
    "        \"\"\"\n",
    "        in order are for shift_x,shift_y\n",
    "        0) 0 0\n",
    "        1) 1 0\n",
    "        2) 0 1\n",
    "        3) 1 1\n",
    "        \"\"\"\n",
    "        def fun_ff():\n",
    "            return initial_masks[:,shape_reshape_cfgs[0].shift_x: self.orig_grid_shape[0]:2,shape_reshape_cfgs[0].shift_y: self.orig_grid_shape[1]:2 ]\n",
    "            \n",
    "        def fun_tf():\n",
    "            return initial_masks[:,shape_reshape_cfgs[1].shift_x: self.orig_grid_shape[0]:2,shape_reshape_cfgs[1].shift_y: self.orig_grid_shape[1]:2 ]\n",
    "            \n",
    "        def fun_ft():\n",
    "            return initial_masks[:,shape_reshape_cfgs[2].shift_x: self.orig_grid_shape[0]:2, shape_reshape_cfgs[2].shift_y: self.orig_grid_shape[1]:2 ]\n",
    "\n",
    "        def fun_tt():\n",
    "            return initial_masks[:,shape_reshape_cfgs[3].shift_x: self.orig_grid_shape[0]:2,shape_reshape_cfgs[3].shift_y: self.orig_grid_shape[1]:2 ]\n",
    "\n",
    "        functions_list=[fun_ff,fun_tf,fun_ft,fun_tt]\n",
    "        return jax.lax.switch(index,functions_list)\n",
    "\n",
    "\n",
    "    def select_recreate_orig_shape(self,arr,index):\n",
    "        \"\"\"\n",
    "        in order are for shift_x,shift_y\n",
    "        0) 0 0\n",
    "        1) 1 0\n",
    "        2) 0 1\n",
    "        3) 1 1\n",
    "        \"\"\"\n",
    "        def fun_ff():\n",
    "            return recreate_orig_shape(arr,self.shape_reshape_cfgs[0],self.to_reshape_back_x,self.to_reshape_back_y )\n",
    "            \n",
    "        def fun_tf():\n",
    "            return recreate_orig_shape(arr,self.shape_reshape_cfgs[1],self.to_reshape_back_x,self.to_reshape_back_y )\n",
    "            \n",
    "        def fun_ft():\n",
    "            return recreate_orig_shape(arr,self.shape_reshape_cfgs[2],self.to_reshape_back_x,self.to_reshape_back_y )\n",
    "\n",
    "        def fun_tt():\n",
    "            return recreate_orig_shape(arr,self.shape_reshape_cfgs[3],self.to_reshape_back_x,self.to_reshape_back_y )\n",
    "\n",
    "        functions_list=[fun_ff,fun_tf,fun_ft,fun_tt]\n",
    "        return jax.lax.switch(index,functions_list)\n",
    "\n",
    "\n",
    "    # @partial(jax.profiler.annotate_function, name=\"in_single_mask_convs\")\n",
    "    # def in_single_mask_convs(self,cat_conv_multi):\n",
    "    #     return remat(nn.Sequential)([\n",
    "    #      Conv_trio(self.cfg,self.features)#no stride\n",
    "    #     ,Conv_trio(self.cfg,self.features)#no stride\n",
    "    #     ,Conv_trio(self.cfg,self.features)])(cat_conv_multi)#no stride\n",
    "\n",
    "   \n",
    "\n",
    "    @partial(jax.profiler.annotate_function, name=\"shape_apply_reshape\")\n",
    "    def shape_apply_reshape(self,resized_image,mask_combined,mask_combined_alt,mask_index,initial_masks):\n",
    "\n",
    "        resized_image=self.select_shape_reshape_operation(resized_image,mask_index,self.shape_reshape_cfgs,divide_sv_grid)\n",
    "        # resized_image=divide_sv_grid(resized_image,shape_reshape_cfg)\n",
    "        mask_combined=self.select_shape_reshape_operation(mask_combined,mask_index,self.shape_reshape_cfgs,divide_sv_grid)\n",
    "        mask_combined_alt=self.select_shape_reshape_operation(mask_combined_alt,mask_index,self.shape_reshape_cfgs,divide_sv_grid)\n",
    "\n",
    "        #needed to know the current id on apply on single area\n",
    "        initial_masks= self.select_id_choose_operation(initial_masks,mask_index,self.shape_reshape_cfgs)\n",
    "        initial_masks= einops.rearrange(initial_masks,'b x y p ->b (x y) p')\n",
    "\n",
    "        losses=batched_v_Apply_on_single_area(self.cfg \n",
    "                                                ,self.dynamic_cfg\n",
    "                                                ,self.rearrange_to_intertwine_einops\n",
    "                                                ,self.dim_stride \n",
    "                                                ,self.current_shape\n",
    "                                                ,self.deconved_shape\n",
    "                                                ,self.translation_val\n",
    "                                                ,self.diameter_x\n",
    "                                                ,self.diameter_y\n",
    "                                                ,self.diameter_x_curr\n",
    "                                                ,self.diameter_y_curr\n",
    "                                                ,self.p_x_y[0]\n",
    "                                                ,self.p_x_y[1]\n",
    "                                                ,self.to_reshape_back_x\n",
    "                                                ,self.to_reshape_back_y\n",
    "                                                )(resized_image,mask_combined,mask_combined_alt,mask_index,initial_masks)\n",
    "        # losses=jnp.ones(2)\n",
    "\n",
    "        # mask_combined_prints=einops.rearrange(mask_combined_prints,'b pp x y ->b pp x y 1')\n",
    "        # mask_combined_print=self.select_recreate_orig_shape(mask_combined_prints,mask_index)\n",
    "        \n",
    "\n",
    "        # return jnp.mean(losses.flatten()) krowa TODO(unhash)\n",
    "        return losses,mask_combined,resized_image,initial_masks\n",
    "        # return jnp.mean(jnp.ones(1))\n",
    "\n",
    "    # def __call__(self, image:jnp.ndarray, mask:jnp.ndarray,deconv_multi:jnp.ndarray,shape_reshape_index:int) -> jnp.ndarray:\n",
    "    @partial(jax.profiler.annotate_function, name=\"De_conv_batched_for_scan\")\n",
    "    @nn.compact\n",
    "    def __call__(self, curried:jnp.ndarray, mask_index:int) -> jnp.ndarray:\n",
    "        # resized_image,mask_combined,mask_combined_alt,initial_masks=curried\n",
    "        resized_image,mask_combined,mask_combined_alt,initial_masks,mask_combined_new,resized_image_new,initial_masks_new=curried\n",
    "        # shape apply reshape \n",
    "        losses,mask_combined_new,resized_image_new,initial_masks_new = self.shape_apply_reshape(resized_image,mask_combined,mask_combined_alt,mask_index,initial_masks)\n",
    "        # image=einops.rearrange(image,'b w (h c)->b w h c',c=1)     \n",
    "        # print(f\"enddd curried_mask {accum_mask.shape} image {image.shape}  deconv_multi {deconv_multi.shape}\")\n",
    "        return ( (resized_image,mask_combined,mask_combined_alt,initial_masks,mask_combined_new,resized_image_new,initial_masks_new) , (losses) )\n",
    "        # return ( (resized_image,mask_combined,mask_combined_alt,initial_masks,mask_combined) , (losses) )\n",
    "\n",
    "\n",
    "class De_conv_batched_multimasks(nn.Module):\n",
    "    \"\"\"\n",
    "    Here we can use multiple masks with diffrent shift configurations\n",
    "    as all configurations True False in shifts in axis is \n",
    "    masks in order are for shift_x,shift_y\n",
    "    0) 0 0\n",
    "    1) 1 0\n",
    "    2) 0 1\n",
    "    3) 1 1\n",
    "    \"\"\"\n",
    "    \n",
    "    cfg: ml_collections.config_dict.config_dict.ConfigDict\n",
    "    dynamic_cfg: ml_collections.config_dict.config_dict.ConfigDict\n",
    "    dim_stride:int\n",
    "    r_x:int\n",
    "    r_y:int\n",
    "    rearrange_to_intertwine_einops:str\n",
    "    translation_val:int\n",
    "    features:int\n",
    "    \n",
    "    def setup(self):\n",
    "        cfg=self.cfg\n",
    "        rss=[self.r_x,self.r_y]\n",
    "        rss[self.dim_stride]=rss[self.dim_stride]-1\n",
    "        self.rss=rss\n",
    "        self.deconved_shape = (self.cfg.batch_size_pmapped,cfg.img_size[2]//2**(cfg.r_x_total -self.r_x),cfg.img_size[3]//2**(cfg.r_y_total-self.r_y),1)\n",
    "        self.deconved_shape_not_batched = (cfg.img_size[2]//2**(cfg.r_x_total -self.r_x),cfg.img_size[3]//2**(cfg.r_y_total-self.r_y),1)\n",
    "        self.current_shape_not_batched=(cfg.img_size[2]//2**(cfg.r_x_total-rss[0]),cfg.img_size[3]//2**(cfg.r_y_total-rss[1]),1)\n",
    "\n",
    "        #we add 1 becouse of batch\n",
    "        dim_stride_curr=self.dim_stride+1        \n",
    "        self.mask_shape = (self.cfg.batch_size_pmapped,cfg.img_size[2]//2**(cfg.r_x_total-rss[0]),cfg.img_size[3]//2**(cfg.r_y_total-rss[1]),self.cfg.num_dim)\n",
    "        mask_shape_list=list(self.mask_shape)\n",
    "        mask_shape_list[dim_stride_curr]=1\n",
    "        self.mask_shape_end=tuple(mask_shape_list)\n",
    "\n",
    "        self.scanned_de_cov_batched=  nn.scan(De_conv_batched_for_scan,\n",
    "                                variable_broadcast=\"params\", #parametters are shared\n",
    "                                split_rngs={'params': False},\n",
    "                                length=self.cfg.masks_num,\n",
    "                                in_axes=(0)\n",
    "                                ,out_axes=(0) )#losses\n",
    "\n",
    "    @partial(jax.profiler.annotate_function, name=\"before_mask_scan_scanning_convs\")\n",
    "    def before_mask_scan_scanning_convs(self,deconv_multi):\n",
    "        return remat(nn.Sequential)([\n",
    "        Conv_trio(self.cfg,self.features)\n",
    "        ,Conv_trio(self.cfg,self.features)\n",
    "        ,Conv_trio(self.cfg,self.features)\n",
    "        ,Conv_trio(self.cfg,self.features)\n",
    "        ,Conv_trio(self.cfg,self.features)\n",
    "        ,Conv_trio(self.cfg,self.features)\n",
    "        ,Conv_trio(self.cfg,self.features)\n",
    "        ,Conv_trio(self.cfg,self.features)\n",
    "        ])(deconv_multi)\n",
    "\n",
    "    @partial(jax.profiler.annotate_function, name=\"scan_over_masks\")\n",
    "    def scan_over_masks(self,resized_image:jnp.ndarray,mask_combined:jnp.ndarray,mask_combined_alt:jnp.ndarray,initial_masks:jnp.ndarray):\n",
    "        curried= (resized_image,mask_combined,mask_combined_alt,initial_masks)\n",
    "        curried,accum= self.scanned_de_cov_batched(self.cfg\n",
    "                                                    ,self.dynamic_cfg\n",
    "                                                   ,self.dim_stride\n",
    "                                                   ,self.r_x\n",
    "                                                   ,self.r_y\n",
    "                                                   ,self.rearrange_to_intertwine_einops\n",
    "                                                   ,self.translation_val\n",
    "                                                    ,self.features\n",
    "                                                    )(curried,jnp.arange(self.cfg.masks_num) )\n",
    "      \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return accum                                            \n",
    "\n",
    "\n",
    "    def get_new_mask_from_probs(self,mask_old:jnp.ndarray,bi_chan_probs:jnp.ndarray):\n",
    "        \"\"\" \n",
    "        given bi channel probs where first channel will be interpreted as probability of being the same id (1 or 0)\n",
    "        as the sv backward the axis; and second channel probability of being the same supervoxel forward the axis \n",
    "        \"\"\"\n",
    "        #we add 1 becouse of batch\n",
    "        dim_stride_curr=self.dim_stride+1\n",
    "        #we get propositions forward and backward od old mask\n",
    "        old_forward=jnp.take(mask_old, indices=jnp.arange(1,self.mask_shape[dim_stride_curr]),axis=dim_stride_curr )\n",
    "        old_back =jnp.take(mask_old, indices=jnp.arange(0,self.mask_shape[dim_stride_curr]),axis=dim_stride_curr )\n",
    "        #in order to get correct shape we need to add zeros to forward\n",
    "        to_end_grid=jnp.ones(self.mask_shape_end)\n",
    "        old_forward= jnp.concatenate((old_forward,to_end_grid) ,axis= dim_stride_curr)\n",
    "\n",
    "        \n",
    "        old_propositions=jnp.stack([old_forward,old_back],axis=-1)# w h n_dim 2\n",
    "        #chosen values and its alternative\n",
    "        bi_chan_probs=v_v_harder_diff_round(bi_chan_probs) \n",
    "\n",
    "        bi_chan_probs=einops.repeat(bi_chan_probs,'bb w h pr->bb w h d pr',d=self.cfg.num_dim)\n",
    "\n",
    "        chosen_values=jnp.multiply(old_propositions,bi_chan_probs)\n",
    "        chosen_values= jnp.sum(chosen_values,axis=-1)\n",
    "        chosen_values_alt=jnp.multiply(old_propositions,(jnp.flip(bi_chan_probs,axis=-1)))\n",
    "        chosen_values_alt= jnp.sum(chosen_values_alt,axis=-1)\n",
    "\n",
    "\n",
    "        mask_combined=einops.rearrange([mask_old,chosen_values],self.rearrange_to_intertwine_einops) \n",
    "        mask_combined_alt=einops.rearrange([mask_old,chosen_values_alt],self.rearrange_to_intertwine_einops)\n",
    "\n",
    "\n",
    "        \n",
    "        return mask_combined,mask_combined_alt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @partial(jax.profiler.annotate_function, name=\"De_conv_batched_multimasks\")\n",
    "    @nn.compact\n",
    "    def __call__(self, image:jnp.ndarray, mask_old:jnp.ndarray,deconv_multi:jnp.ndarray,initial_masks:jnp.ndarray) -> jnp.ndarray:\n",
    "        \n",
    "        #some primary convolutions\n",
    "        resized_image=v_image_resize(image,self.deconved_shape_not_batched,\"linear\" )\n",
    "        #adding information about image as the first channel\n",
    "        deconv_multi= jnp.concatenate([v_image_resize(image,self.current_shape_not_batched,\"linear\" ),deconv_multi],axis=-1)\n",
    "        deconv_multi=self.before_mask_scan_scanning_convs(deconv_multi)\n",
    "        #resisizing image to the deconvolved - increased shape\n",
    "        # image_resized= einops.rearrange(image,'b w h c->b w (h c)',c=1)\n",
    "        # resized_image= jax.image.resize(image, self.deconved_shape, \"linear\")\n",
    "        \n",
    "        # resized_image= jax.image.resize(image[0,:,:,:], (self.deconved_shape[1],self.deconved_shape[2] ), \"linear\")\n",
    "        \n",
    "\n",
    "        #getting new mask thanks to convolutions...\n",
    "        mask_new_bi_channel=remat(nn.Conv)(2, kernel_size=(5,5))(deconv_multi)\n",
    "        mask_new_bi_channel=nn.softmax(mask_new_bi_channel,axis=-1)\n",
    "\n",
    "        #intertwine old and new mask - so a new mask may be interpreted basically as interpolation of old\n",
    "        mask_combined,mask_combined_alt=self.get_new_mask_from_probs(mask_old,mask_new_bi_channel)\n",
    "        #we want the masks entries to be as close to be 0 or 1 as possible - otherwise feature variance and \n",
    "        #separability of supervoxels will be compromised\n",
    "        # having entries 0 or 1 will maximize the term below so we negate it for loss\n",
    "        # rounding_loss_val=jnp.mean(jnp.array([rounding_loss(mask_combined),rounding_loss(mask_combined_alt)]))\n",
    "        #making the values closer to 1 or 0 in a differentiable way\n",
    "        mask_combined=v_v_harder_diff_round(mask_combined)       \n",
    "        mask_combined_alt=v_v_harder_diff_round(mask_combined_alt)       \n",
    "\n",
    "        #getting the increased size for a next iteration\n",
    "        deconv_multi=remat(De_conv_not_sym)(self.cfg,self.features,self.dim_stride)(deconv_multi)\n",
    "\n",
    "\n",
    "        #we scan over using diffrent shift configurations\n",
    "        losses=self.scan_over_masks(resized_image,mask_combined,mask_combined_alt,initial_masks) \n",
    "\n",
    "        #reducing the scanned ...        \n",
    "        losses= jnp.mean(losses.flatten())#+rounding_loss_val\n",
    "\n",
    "        #consistency_loss,rounding_loss,feature_variance_loss,edgeloss,average_coverage_loss,=consistency_loss,rounding_loss,feature_variance_loss,edgeloss,average_coverage_loss,consistency_between_masks_loss=losses\n",
    "\n",
    "        # return deconv_multi,masks,out_image,jnp.mean(jnp.stack([consistency_loss, rounding_loss,feature_variance_loss,consistency_between_masks_loss ]).flatten())\n",
    "        return deconv_multi,mask_combined,losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class De_conv_3_dim(nn.Module):\n",
    "    \"\"\"\n",
    "    applying De_conv_batched_multimasks for all axes\n",
    "    r_x,r_y tell about \n",
    "    \"\"\"\n",
    "    cfg: ml_collections.config_dict.config_dict.ConfigDict\n",
    "    dynamic_cfg: ml_collections.config_dict.config_dict.ConfigDict\n",
    "\n",
    "    features: int\n",
    "    r_x:int\n",
    "    r_y:int\n",
    "    translation_val:int\n",
    "\n",
    "    @partial(jax.profiler.annotate_function, name=\"De_conv_3_dim\")\n",
    "    @nn.compact\n",
    "    def __call__(self, image:jnp.ndarray, masks:jnp.ndarray,deconv_multi:jnp.ndarray,initial_masks:jnp.ndarray) -> jnp.ndarray:\n",
    "        \n",
    "        deconv_multi,masks,losses_1=remat(De_conv_batched_multimasks)(self.cfg\n",
    "                                   ,self.dynamic_cfg\n",
    "                                   ,0#dim_stride\n",
    "                                   ,self.r_x\n",
    "                                   ,self.r_y-1\n",
    "                                   ,'f bb h w cc->bb (h f) w cc'#rearrange_to_intertwine_einops\n",
    "                                   ,self.translation_val\n",
    "                                   ,self.features\n",
    "                                   )(image,masks,deconv_multi,initial_masks)\n",
    "        \n",
    "        deconv_multi,masks,losses_2=remat(De_conv_batched_multimasks)(self.cfg\n",
    "                                   ,self.dynamic_cfg\n",
    "                                   ,1#dim_stride\n",
    "                                   ,self.r_x\n",
    "                                   ,self.r_y\n",
    "                                   ,'f bb h w cc->bb h (w f) cc'#rearrange_to_intertwine_einops\n",
    "                                   ,self.translation_val\n",
    "                                   ,self.features\n",
    "                                   )(image,masks,deconv_multi,initial_masks)\n",
    "\n",
    "        #consistency_loss,rounding_loss,feature_variance_loss,edgeloss,average_coverage_loss,=consistency_loss,rounding_loss,feature_variance_loss,edgeloss,average_coverage_loss,consistency_between_masks_loss=losses\n",
    "        losses= jnp.mean(jnp.stack([losses_1,losses_2],axis=0),axis=0)\n",
    "\n",
    "\n",
    "        return (deconv_multi,masks,losses)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "scale=2\n",
    "\n",
    "def masks_with_boundaries(mask_combined_new,initial_masks_new,resized_image_new,index):\n",
    "    image_to_disp=resized_image_new[0,index,:,:,0]\n",
    "    mask_0=filter_mask_of_intrest(mask_combined_new[0,index,:,:,:],initial_masks_new[0,index,:])\n",
    "    shapp=image_to_disp.shape\n",
    "    image_to_disp_big=jax.image.resize(image_to_disp,(shapp[0]*scale,shapp[1]*scale), \"linear\")     \n",
    "    shapp=mask_0.shape\n",
    "    mask_0_big=jax.image.resize(mask_0,(shapp[0]*scale,shapp[1]*scale), \"nearest\")  \n",
    "    with_boundaries=mark_boundaries(image_to_disp_big, np.round(mask_0_big).astype(int) )\n",
    "    with_boundaries= np.array(with_boundaries)\n",
    "    with_boundaries= einops.rearrange(with_boundaries,'w h c->w (h c)')\n",
    "    to_dispp_svs=with_boundaries\n",
    "    return to_dispp_svs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff130527790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "img=masks_with_boundaries(mask_combined_new,initial_masks_new,resized_image_new,128)\n",
    "# img2 = img[:,:,::-1]\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "img=masks_with_boundaries(mask_combined_new,initial_masks_new,resized_image_new,33)\n",
    "# img2 = img[:,:,::-1]\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 0,  1, 15, 16], dtype=int32),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_mask_of_intrest(mask_combined_new[0,1,:,:,:],initial_masks_new[0,1,:])\n",
    "(losses[0,:]==jnp.max(losses[0,:])).nonzero()#128\n",
    "(losses[0,:]==jnp.min(losses[0,:])).nonzero()#0,  1, 15, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.set_cmap(\"gray\")\n",
    "plt.imshow(mask_combined_new[0,1:,:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with jax.profiler.trace(\"/workspaces/Jax_cuda_med/data/profiler_data\", create_perfetto_link=True):\n",
    "  x = random.uniform(random.PRNGKey(0), (100, 100))\n",
    "  jnp.dot(x, x).block_until_ready() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
